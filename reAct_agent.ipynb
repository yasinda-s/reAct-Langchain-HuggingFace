{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23c40b9-e5c2-4d8d-9a4f-c35258eacac5",
   "metadata": {},
   "source": [
    "Reference - https://medium.com/@jainashish.079/build-llm-agent-combining-reasoning-and-action-react-framework-using-langchain-379a89a7e881"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6baa80d",
   "metadata": {},
   "source": [
    "Most applications will require the LLM not to do only reasoning, even reasoning steps might requires calling multiple external data sources and applications. One of the techniques or framework is called ReAct (Reasoning and actions) in which LLM can plan out these reasoning steps and execute those steps.\n",
    "\n",
    "ReAct is technique which enable LLMs to do reasoning and take task specific actions. It combines chain of thought reasoning with action planning.\n",
    "ReAct agents can perform 3 major actions : search about entities, look up strings, and finish current task with an answer once found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f315d6-e844-4ddb-ad5d-5c0f5ea1577a",
   "metadata": {},
   "source": [
    "Langchain's DocStoreExplorer agent interacts with a document storage system (like Wikipedia), using two specific tools , a Search tool and a Lookup tool. The Search tool is responsible for locating a document, whereas the Lookup tool retrieves a term from the most recently discovered document. We can initialize this doc store with Wikipedia as a document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ded7d-cb7e-4da9-8fe3-31382a329ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "from langchain.docstore import Wikipedia\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from keys import HUGGING_FACE_KEY\n",
    "\n",
    "# If loading from OpenAI using env\n",
    "os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# If loading from Hugging Face using env\n",
    "os.getenv('HUGGING_FACE_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f37b55f-181a-4c45-85af-e72e4f821837",
   "metadata": {},
   "source": [
    "# 1. Using AutoTokenizer, AutoModelForCausalLM Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba3cec-c9e2-4c66-b3ab-29cb33762400",
   "metadata": {},
   "source": [
    "## Downloads LLM + Tokenizer locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe7c5c-12e4-4189-a33c-d4bbcc04694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#If using HuggingFace model for the first time\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", cache_dir=\"D:/HuggingFaceTokenizerCache/\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", cache_dir=\"D:/HuggingFaceModelCache/\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D:/HuggingFaceTokenizerCache/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"D:/HuggingFaceModelCache/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873\", low_cpu_mem_usage=True)\n",
    "\n",
    "model_inputs = tokenizer([\"Who is the current president of Germany?\"], return_tensors=\"pt\") #tokenize the string\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100) #use model to generate 100 new tokens based on the tokenized input\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] #use the same tokenizer to decode the generated output from model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5202f9c-f69a-4eb6-918b-a869052e596e",
   "metadata": {},
   "source": [
    "# 2. Using HuggingFaceEndpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300e32b-2cda-49b1-8e36-a7db1cfec012",
   "metadata": {},
   "source": [
    "## No local downloads of LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ac3566-c92f-4123-a79e-23917bb1fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    huggingfacehub_api_token=HUGGING_FACE_KEY,\n",
    "    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    top_k=50,\n",
    "    top_p=0.8,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ed6ab-b742-45a4-b830-1ee806770cb3",
   "metadata": {},
   "source": [
    "## Setting up reAct tools needed for agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d3f664-886c-4ee4-bd4b-1eb8b89379a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0e33f-8fa8-4d15-ac79-310af4cc48cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tools = load_tools([\"ddg-search\"])\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1959c09-8bf5-4c9e-94b1-09f240ca78d0",
   "metadata": {},
   "source": [
    "## Create and run reAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62172490-7db3-45e1-b1e3-c74ecd6e2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354cebc-e123-4076-8cd6-583742db25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"What does Dale Steyn and Kumar Sangakkara have in common?\", \"use_gpu\": True})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
